---
title: "Analyzing the effect of the interaction between attachment style and internalizing behavior on school connectedness"
author: "Ian Douglas"
date: "12/12/2019"
output:
  html_document:
    number_sections: yes
    toc: yes
    df_print: paged
    toc_float:
      collapsed: no
      smooth_scroll: yes
---
- Full Github repositiory for this project's code and figures <a href="http://github.com/ijd2109/fragileFamilies">here</a>.
- <a href="http://fragilefamilies.princeton.edu">Data</a> sourced from and maintained by Princeton and Columbia Universities.


### Analysis Plan
1. Compute correlations between the outcome, *age 15 connectedness at school (CS15)*, and the predictors: *internalizing behaviors (IB)* and *age 9 (CS9)*.
2. Examine mean differences between the groups, and variance ratios.
3. Due to the "rule of thumb", conclude that mean differences do not differ, but variance homogeneity is likely violated.
4. Conduct Welch's test to control Type I error rate while assessing the effect of the grouping variable.
5. The mean differences were found to be non-significant, but the hypothesis purports a crossing-type interaction, so plot the groups on the dimensions of IB (binned into high and low categories) and CS15.
6. The graphical evidence is clear that an interaction exists, and that a meaningful grouping of subjects should be 'avoidant' and 'not avoidant'. 
7. Fit the interaction model: predict CS15 from the Group-by-IB interaction, controlling for the continuous covariate CS9.
8. Report the F-statistic and p-value to indicate the significance of the model, as well as the significant (and standardized) Beta coefficients as a measure of effect size for the significant covariate and the interaction.
9. Interpret the conclusion of the model: the significant interaction shows that after controlling for CS9, the negative effect of IB on CS15 is worsened for subjects exhibiting avoidant attachment with caregivers at age three.

### Load libraries
```{r, results='hide', message=FALSE}
require(effsize)
require(tidyverse)
require(car)
require(pscl)
```

### Read in data
```{r}
# convert from Tibble to data.frame
df = as.data.frame(readRDS("../../data/processed/modelData.rds")) # output of wrangling script
rownames(df) <- df$idnum
# finally, delete the rows for whom there are NA in the outcome column (only)
df = df %>%
  filter(!is.na(target))
```

# Compute correlations.
## First between IB and CS9 with CS15.
```{r}
# the continuous predictor
cor.test(df$target, df$internalizingSum, use = "complete.obs")
# the year9 observation of outcome (to control for)
cor.test(df$target, df$yr9_CNCT_Sum, use = "complete.obs")
# testing multicolinearity
cor.test(df$internalizingSum, df$yr9_CNCT_Sum, use = "complete.obs")
```
 Both predictors are significantly correlated (though weakly) with the outcome. Weak multicolinearity present between them as well.

# Standardized mean differences and variance ratios.
## Pairwise mean comparisons. 
### First between the three groups individually on the outcome (CS15)
```{r}
secure_avoidant = df %>% filter(resistant == 0)
resistant_secure = df %>% filter(avoidant == 0)
resist_avoid = df %>% filter(secure == 0)
contrast_list = list(secure_avoidant, resistant_secure, resist_avoid)
effsize::cohen.d(d = contrast_list[[1]]$target, as.factor(contrast_list[[1]]$secure),
                 na.rm = TRUE)
effsize::cohen.d(d = contrast_list[[2]]$target, as.factor(contrast_list[[2]]$secure),
                 na.rm = TRUE)
effsize::cohen.d(d = contrast_list[[3]]$target, as.factor(contrast_list[[3]]$avoidant),
                 na.rm = TRUE)
```

### Now on covariate IB
```{r}
# secure v avoidant
effsize::cohen.d(d = contrast_list[[1]]$internalizingSum,
                 as.factor(contrast_list[[1]]$secure),
                 na.rm = TRUE)
# secure v resistant
effsize::cohen.d(d = contrast_list[[2]]$internalizingSum,
                 as.factor(contrast_list[[2]]$secure),
                 na.rm = TRUE)
# avoidant v resistant
effsize::cohen.d(d = contrast_list[[3]]$internalizingSum,
                 as.factor(contrast_list[[3]]$avoidant),
                 na.rm = TRUE)
```

None of the means differ on average between groups.

## Variance ratios for between the avoidant and non-avoidant groups
### First for the outcome CS15
```{r}
# Avoidant groups relative to not, for age 15 connectedness
var(na.omit((filter(df, avoidant == 1))$target))/var(na.omit((filter(df, avoidant == 0))$target))
```
### Age 9 connectedness
```{r}
var(na.omit((filter(df, avoidant == 1))$yr9_CNCT_Sum))/var(na.omit((filter(df, avoidant == 0))$yr9_CNCT_Sum))
```
### Covariate IB
```{r}
var(na.omit((filter(df, avoidant == 1))$internalizingSum))/var(na.omit((filter(df, avoidant == 0))$internalizingSum))
```

Results indicate a degree of imbalance that is concerning, according to Rubin's (2001) guidelines, for CS measured at age 9 and 15 (but not the covariate IB). The scenario suggests that the group with a much smaller sample size, those of avoidant attachment styles, has the much higher variance in the domain of school connectedness at ages 9 and 15 than the other two groups. Thus, conducting traditional ANOVA is more likely to increase the risk of comitting Type I error. Instead, to test the significance of the grouping factor on CS15, I will use Welch's test.

## Visualization of the distribution of the outcome measure within each group
### Retaining all three groups:
```{r,eval=FALSE}
# A density plot wil visualize the means and variances for each group as well:
pdf(file="../../plots/groupDensities.pdf", width = 12, height = 8, bg = "white")
densityPlot(df$target, g = df$ch3att_codeabc, 
            legend = list(location = "topleft", title = 'Attachment Style'),
            xlab = "Connectedness at School", adjust = c(.7, 2, 2))
abline(v=mean(na.rm=TRUE,(df%>%filter(ch3att_codeabc=="avoidant"))$target), lwd =2)
abline(v=mean(na.rm=TRUE,(df%>%filter(ch3att_codeabc=="secure"))$target),lty=2,col=4,lwd =2)
abline(v=mean(na.rm=TRUE,(df%>%filter(ch3att_codeabc=="resistant"))$target),lty=3,col=6,lwd =2)
dev.off()
```

<img src="../../plots/groupDensities.pdf" alt="dplt"  width="720" height="480">

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap=c("your caption"), echo=FALSE, eval = TRUE}
#knitr::include_graphics("../../plots/groupDensities.pdf")
```

### And between two groups
```{r, eval=FALSE}
density_Data = df %>% mutate_at(vars(avoidant), ~as.factor(ifelse(.==1,"avoidant","other")))
pdf(file="../../plots/binTWO_groupDensities.pdf", width = 12, height = 8, bg = "white")
densityPlot(density_Data$target, g = density_Data$avoidant, 
            legend = list(location = "topleft", title = 'Avoidant'),
            xlab = "Connectedness at School", adjust = c(.7, 2.3))
abline(v = mean(na.rm=TRUE,(density_Data%>%filter(avoidant=="avoidant"))$target), lwd =2)
abline(v=mean(na.rm=TRUE,(density_Data%>%filter(avoidant=="other"))$target),lty=2,col=4,lwd =2)
dev.off()
```

<img src="../../plots/binTWO_groupDensities.pdf" alt="dplt2"  width="720" height="480">

```{r, out.width="0.3\\linewidth", include=TRUE, fig.align="center", fig.cap=c("your caption"), echo=FALSE, eval = TRUE}
#knitr::include_graphics("../../plots/binTWO_groupDensities.pdf")
```

# Welch's one-way ANOVA corection for heteroskedasticity.
## Comparing the avoidant to the resistant group
```{r}
oneway.test(target ~ as.factor(avoidant), data = resist_avoid)
```

## Comparing the avoidant to the secure group
```{r}
oneway.test(target ~ as.factor(avoidant), data = secure_avoidant)
```

## Comparing the avoidant to the non-avoidant group
```{r}
oneway.test(target ~ as.factor(avoidant), data = df)
```

## All groups (omnibus):
```{r}
oneway.test(target ~ ch3att_codeabc, data = df)
```

Conclusion: the groups alone do not explain a significant proportion in the variance of CS15.

# Plot the hypothesized interaction.
## At low and high levels of internalizing, plot group means for target
```{r, eval=FALSE}
# reformat data for plotting purposes.
plt_data = df %>%
  filter(!is.na(internalizingSum)) %>%
  group_by(ch3att_codeabc) %>%
  mutate(internalizing = ifelse(internalizingSum <= median(internalizingSum,na.rm = TRUE),
                                  "low","high")) %>%
  group_by(ch3att_codeabc, internalizing) %>%
  summarize_at(vars(internalizingSum, target), 
               list(~mean(., na.rm = TRUE), ~sd(., na.rm = TRUE))) %>%
  ungroup() %>%
  rename_at(vars(ch3att_codeabc), ~("Group"))

# generate plot:
plt <- ggplot(plt_data, aes(x = internalizing, y = target_mean, group = Group)) +
  geom_point(aes(color = Group)) +
  geom_line(aes(color = Group)) +
  ggtitle(label = "Mean Connectedness at School by Group and Level of Internalizing Behavior")

# display plot:
plt
```
```{r, eval=FALSE,echo=FALSE}
ggsave(plt, device = "pdf", file="../../plots/interaction_plot.pdf",
       width = 12, height = 8)
```
```{r, eval=FALSE,echo=FALSE}
save(plt, file = "../../plots/layers/plt_ggplotlayers_pdfinteraction_plot.RData")
```
```{r,eval=TRUE,echo=FALSE}
load("../../plots/layers/plt_ggplotlayers_pdfinteraction_plot.RData")
plt
```

## Binning into avoidant and non-avoidant groups, as per theorized interaction
```{r, eval=FALSE}
# format data for plotting purposes
plt_data2 = df %>% 
  mutate_at("avoidant", ~as.factor(ifelse(. == 1, "True", "False"))) %>%
  filter(!is.na(internalizingSum)) %>%
  mutate(internalizing = ifelse(internalizingSum <= median(internalizingSum,na.rm = TRUE),
                                  "low","high")) %>%
  group_by(avoidant, internalizing) %>%
  summarize_at(vars(internalizingSum, target), 
               list(~mean(., na.rm = TRUE), ~sd(., na.rm = TRUE))) %>%
  ungroup()

# generate plot:
plt2 <- ggplot(plt_data2, aes(x = internalizing, y = target_mean, group = avoidant)) +
  geom_point(aes(color = avoidant)) +
  geom_line(aes(color = avoidant)) +
  ggtitle(label = "Mean Connectedness at School by Group and Level of Internalizing Behavior")

# display plot:
plt2
```
```{r, eval=FALSE,echo=FALSE}
ggsave(plt2, device = "pdf", file="../../plots/2group_int_plot.pdf",
       width = 12, height = 8)
```
```{r, eval=FALSE,echo=FALSE}
save(plt2, file = "../../plots/layers/plt2_ggplotlayers_2group_int_plot.RData")
```
```{r,eval=TRUE,echo=FALSE}
load("../../plots/layers/plt2_ggplotlayers_2group_int_plot.RData")
plt2
```

This crossing type interaction is a possible explanation for the undeteceted mean differences, and insignificant main effect of group (both when binning created two- and three-groups). This provides graphical evidence that an interaction model may still be appropriate, and subsequent modeling (below) will follow from this conclusion.

# Regression Analyses
## Fit the model
```{r}
# note, the interaction is drawn between internalization and the dummy code for avoidant
# Thus, implicitly, there are two groups in this model.

model = lm(target ~ yr9_CNCT_Sum + # year 15 connectedness controlling for that of year 9
             internalizingSum*avoidant, # the int. btwn internalizing and avoidant
           data = df) # the full data.
#saveRDS(model, "../../output/interactionModel_TWOGrp.rds")
```
```{r, eval=FALSE,echo=FALSE}
saveRDS(model, "../../output/interactionModel_TWOGrp.rds")
```
```{r, eval=TRUE, echo=FALSE}
model = readRDS("../../output/interactionModel_TWOGrp.rds")
```

## Regression estimates
```{r}
summary(model)
```

## ANOVA table:
```{r}
summary(aov(model))
```

# Diganostics
## Analyze the constant variance assumption
```{r}
plot(model$fitted.values, model$residuals, type = "h", 
     main = "Fitted Values - Residuals Plot")
abline(h = 0, lty = 3, col="grey")
```

 Conclusions: There is a very small fraction of outliers relative to the sample size. As a result these participants will be dropped before fitting the model again. Second, the variance appears to decline with higher values of $\hat{Y_i}$. The variance is also extremely negatively skewed.
 
## Mathematically analyze heteroscedasticity
```{r}
df = df %>%
  mutate(targetQuantile =
           factor(cut(target, quantile(target, probs=c(0, .33, .66, 1)),
                      include.lowest = TRUE),
                  labels = c("low","med","high")))
df %>% group_by(targetQuantile) %>%
  summarize_at(vars(target), 
               list("mean"=mean,"var"=var,"n"=length,
                    "equalMin"= ~sum(. == min(df$target)),
                    "equalMax" = ~sum(. == max(df$target))))
```

This reveals the (potentially: only) issue. The entire highest third of the data have responded to the survey by choosing the highest score possible. Therefore, the modelling approach will change to account for the probability of choosing the highest score on the survey.

# Fit the zero-inflated model

  To control for the ceiling effect by which participants were much more likely to select the maximum response on the survey, I will reverse code the sums so that they reflect a new construct that will be positively skewed (rather than negatively skewed), and then offset by a constant so that the minimum is zero. It will thus represent **dis**-connection at school, rather than connectedness, and follow a zero-inflated Poisson distribution. As such, my modeling approach will include a zero-inflated negative binomial model.

## Transforming the target variable:
```{r}
df2 = df %>%
  # subtracting the maximum will reverse code it and shift the minimum to zero.
  mutate(disconnection = 16 - round(target)) # round to ensure no non-positive vals
rownames(df2) <- df2$idnum
```

## Check that the correlations have been reversed:
```{r}
cor(df2$disconnection, df2$target, use = 'complete.obs') # should be -1
# the continuous predictor
cor.test(df2$disconnection, df2$internalizingSum, use = "complete.obs")
# the year9 observation of outcome (to control for)
cor.test(df2$disconnection, df2$yr9_CNCT_Sum, use = "complete.obs")
```

#### Good, these correlations are exactly identical as with the previous coding.

## Visualize the new distribution:
```{r}
#plot.window(xlim = c(0,4), ylim = c(0,.4), xaxs="i")
plot(density(df2$disconnection, from = min(df2$disconnection),
             to = max(df2$disconnection), bw = .7, kernel = 'g'),
     main="Aggregate Distribution of Disconnectedness", xaxs = "i")
abline(v = mean(df2$disconnection), lty = 2)
text(x = mean(df2$disconnection), y = .075, 
     labels = expression(bar("Y")), pos = 4)
```

## Fit the zero inflated model
```{r}
zeroInflMod = zeroinfl(
  disconnection ~ yr9_CNCT_Sum + internalizingSum*avoidant | 
    socialSumTOTAL,
  data = df2, na.action = na.omit, dist = 'negbin'
)
summary(zeroInflMod)
```

## Subset the data to remove NA
```{r}
df3 = na.omit(df2 %>%
                select(idnum, target, disconnection, 
                       avoidant, yr9_CNCT_Sum, internalizingSum)
) %>%
  mutate(non_zero = ifelse(disconnection == 0, 0, 1))
rownames(df3) <- df3$idnum
hurdle_fn = function(data) {
  zeroMod = glm(non_zero ~ yr9_CNCT_Sum + internalizingSum - 1,
                    data = data, family = 'quasibinomial')
  data$nz_Prob = predict(test.zeroMod, type ="resp") # output the probability
  hurdleMod = glm(disconnection ~ nz_Prob+yr9_CNCT_Sum + internalizingSum * avoidant, 
                  data = subset(data, non_zero == 1), 
                  family = Gamma(link = log))
  part2_mod = glm(disconnection ~ nz_Prob + yr9_CNCT_Sum + internalizingSum * avoidant, 
                  data = subset(data, non_zero == 1), 
                  family = 'gaussian')
  return(list("hurdle"=hurdleMod, "zeroMod" = zeroMod, "twopm"=part2_mod))
}

newModels = hurdle_fn(df3)
```

```{r}
model2 = newModels$hurdle
predict(model2, type = "terms")
```


## Fit the hurdle model for the continuous 'clumped' outcome.
```{r}
model2 = hurdle(disconnection ~ yr9_CNCT_Sum + internalizingSum * avoidant |
                    yr9_CNCT_Sum + internalizingSum + ch3att_codeabc,
                na.action = na.omit, data = df2)
```


## Identify influential outliers
```{r}
infl = rownames(influencePlot(model))
infl
```

## Analyze the normality assumption of the residuals
```{r}
qqPlot(resid(model))
```

As expected from the fitted value X residual plot, the qq plot also shows that the residuals are not normally distributed. But this would be expected anyway if the errors are not heterscedastic anyway.

## Respecify without outliers and repeat
```{r}
df2 = df[!df$idnum %in% infl, ]
model2 = lm(target ~ yr9_CNCT_Sum + internalizingSum * avoidant, 
            data = df2)
```

## Plot the fitted values against the residuals
```{r}
plot(model2$fitted.values, model2$residuals, type = "h",
     main = "Fitted Values and Residuals")
abline(h = 0, lty = 3, col="grey")
```

 This seems to have slightly flattened the decreasing trend, but not entirely. 
## Render the scale-location plot
```{r}
plot(model2, 3)
```

#Boxcox transformation would be next if de-censoring the data doesn't help.


